# Qwen Image LoRA Training - Optimal Defaults Configuration
# This file contains the recommended default values for Qwen Image LoRA training
# Based on documentation examples and best practices

# Model Settings - REQUIRED FIELDS MUST BE SET BY USER
dit = ""  # REQUIRED: Path to DiT checkpoint (qwen_image_bf16.safetensors)
vae = ""  # REQUIRED: Path to VAE checkpoint (diffusion_pytorch_model.safetensors)
text_encoder = ""  # REQUIRED: Path to Qwen2.5-VL checkpoint
dataset_config = ""  # REQUIRED: Path to dataset TOML configuration

dit_dtype = "bfloat16"  # HARDCODED: Must be bfloat16 for Qwen Image
fp8_vl = true  # RECOMMENDED: Saves ~8GB VRAM with minimal quality loss
fp8_base = false  # Optional: FP8 for DiT model, saves ~12GB VRAM
fp8_scaled = false  # REQUIRED when fp8_base=true, better quality than standard FP8
blocks_to_swap = 0             # 0 = disabled. 16=save ~8GB VRAM, 45=save ~30GB VRAM (requires 64GB+ RAM)

# Additional Model Settings  
guidance_scale = 1.0          # Classifier-free guidance scale. 1.0 = default, higher = stronger prompt adherence
img_in_txt_in_offloading = false  # Memory optimization for mixed image-text inputs

# Flow Matching Settings
timestep_sampling = "shift"
discrete_flow_shift = 3.0
weighting_scheme = "none"
logit_mean = 0.0
logit_std = 1.0
mode_scale = 1.29

# Advanced Timestep Parameters (usually leave as defaults)
sigmoid_scale = 1.0          # Scale factor for sigmoid timestep sampling. Only used with 'sigmoid' method
min_timestep = 0            # 0 = no minimum constraint. 0-999 = constrain minimum timestep
max_timestep = 0            # 0 = no maximum constraint. 1-1000 = constrain maximum timestep 
preserve_distribution_shape = false  # Preserve original distribution when using min/max constraints

# Debugging
show_timesteps = ""

# Training Settings
sdpa = true
flash_attn = false
sage_attn = false
xformers = false
flash3 = false               # EXPERIMENTAL: FlashAttention 3, not confirmed to work with Qwen Image
split_attn = false           # REQUIRED if using flash_attn/sage_attn/xformers/flash3
max_train_steps = 1600
max_train_epochs = 16
max_data_loader_n_workers = 2
persistent_data_loader_workers = true
seed = 42
gradient_checkpointing = true
gradient_accumulation_steps = 1

# Optimizer Settings
optimizer_type = "adamw8bit"
learning_rate = 1e-4
max_grad_norm = 1.0
lr_scheduler = "constant"
lr_warmup_steps = 0  # 0 = no warmup. Integer = steps, float <1 = ratio of total steps
lr_decay_steps = 0   # 0 = no decay. Integer = steps, float <1 = ratio of total steps
lr_scheduler_num_cycles = 1
lr_scheduler_power = 1.0
lr_scheduler_timescale = 0      # 0 = auto (uses warmup steps). Advanced parameter
lr_scheduler_min_lr_ratio = 0.0  # 0.0 = can reach zero LR. 0.1 = minimum 10% of initial LR
lr_scheduler_type = ""
lr_scheduler_args = ""

# Network Settings
no_metadata = false
network_weights = ""
network_module = "networks.lora_qwen_image"
network_dim = 32
network_alpha = 1.0
network_dropout = 0.0  # 0.0 = no dropout. 0.1 = 10% dropout for regularization
network_args = ""
training_comment = ""
dim_from_weights = false
scale_weight_norms = 0.0  # 0.0 = disabled. 1.0+ = scale weights to prevent exploding gradients
base_weights = ""
base_weights_multiplier = 1.0  # Only used if base_weights is specified. 1.0 = full strength

# Save/Load Settings
output_dir = ""
output_name = ""
resume = ""
save_every_n_epochs = 1
save_every_n_steps = 0        # 0 = disabled. Positive integer = save every N steps
save_last_n_epochs = 0       # 0 = keep all. Positive integer = keep only last N epoch checkpoints
save_last_n_epochs_state = 0 # 0 = keep all. Positive integer = keep only last N epoch states
save_last_n_steps = 0        # 0 = keep all. Positive integer = keep only last N step checkpoints
save_last_n_steps_state = 0  # 0 = keep all. Positive integer = keep only last N step states
save_state = false
save_state_on_train_end = false

# Caching Settings - Latents
caching_latent_device = "cuda"
caching_latent_batch_size = 4
caching_latent_num_workers = 8
caching_latent_skip_existing = true
caching_latent_keep_cache = true
caching_latent_debug_mode = ""
caching_latent_console_width = 80
caching_latent_console_back = ""
caching_latent_console_num_images = 0  # 0 = no limit. Positive integer = max images in debug console

# Caching Settings - Text Encoder
caching_teo_text_encoder = ""
caching_teo_device = "cuda"
caching_teo_fp8_vl = true
caching_teo_batch_size = 16
caching_teo_num_workers = 8
caching_teo_skip_existing = true
caching_teo_keep_cache = true

# Accelerate Launch Settings
mixed_precision = "bf16"    # RECOMMENDED for Qwen Image. fp16 may cause instability, fp8 not supported
dynamo_backend = "NO"       # NO = disabled (recommended). Use INDUCTOR for PyTorch 2.0+ optimization

# Logging Settings
logging_dir = ""
log_with = ""
log_prefix = ""
log_tracker_name = ""
wandb_run_name = ""
log_tracker_config = ""
wandb_api_key = ""
log_config = false

# DDP Settings
ddp_timeout = 0  # 0 = use default (30min). Positive integer = timeout in minutes for distributed training
ddp_gradient_as_bucket_view = false
ddp_static_graph = false

# Sample Generation  
sample_every_n_steps = 0   # 0 = disabled. 100-500 recommended for step-based sampling
sample_every_n_epochs = 0  # 0 = disabled. 1-4 recommended for epoch-based sampling
sample_at_first = false
sample_prompts = ""

# Metadata Settings
metadata_author = ""
metadata_description = ""
metadata_license = ""
metadata_tags = ""
metadata_title = ""

# HuggingFace Settings
huggingface_repo_id = ""
huggingface_token = ""
huggingface_repo_type = ""
huggingface_repo_visibility = ""
huggingface_path_in_repo = ""
save_state_to_huggingface = false
resume_from_huggingface = false
async_upload = false